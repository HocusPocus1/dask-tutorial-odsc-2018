{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel and Distributed Machine Learning\n",
    "\n",
    "[Dask-ML](https://dask-ml.readthedocs.io) has resources for parallel and distributed machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Scaling\n",
    "\n",
    "There are a couple of distinct scaling problems you might face.\n",
    "The scaling strategy depends on which problem you're facing.\n",
    "\n",
    "1. Large Models: Data fits in RAM, but training takes too long. Many hyperparameter combinations, a large ensemble of many models, etc.\n",
    "2. Large Datasets: Data is larger than RAM, and sampling isn't an option.\n",
    "\n",
    "![](static/ml-dimensions-color.png)\n",
    "\n",
    "* For in-memory problems, just use scikit-learn (or your favorite ML library).\n",
    "* For large models, use distributed joblib and your favorite scikit-learn estimator\n",
    "* For large datasets, use `dask_ml` estimators or load the data with Dask and pass it off to a distributed machine learning library (e.g. XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn in 5 Minutes\n",
    "\n",
    "Scikit-Learn has a nice, consistent API.\n",
    "\n",
    "1. You instantiate an `Estimator` (e.g. `LinearRegression`, `RandomForestClassifier`, etc.). All of the models *hyperparameters* (user-specified parameters, not the ones learned by the estimator) are passed to the estimator when it's created.\n",
    "2. You call `estimator.fit(X, y)` to train the estimator.\n",
    "3. Use `estimator` to inspect attributes, make predictions, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=4, random_state=0)\n",
    "X[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit a [Suppport Vector Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the estimator and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC(random_state=0, gamma='scale')\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the learned attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models have *hyperparameters*. They affect the fit, but are specified up front instead of learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC(C=0.00001, shrinking=False, random_state=0, gamma='scale')\n",
    "estimator.fit(X, y)\n",
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "There are a few ways to learn the best hyperparameters while training. One is `GridSearchCV`.\n",
    "As the name implies, this does a brute-force search over a grid of hyperparameter combinations.\n",
    "\n",
    "## Single-machine parallelism with scikit-learn\n",
    "\n",
    "![](static/sklearn-parallel.png)\n",
    "\n",
    "As you may suspect, this brute-force search can be done in parallel. Each of the `(hyperparameter, cv-split)`\n",
    "combinations in a grid search can be done independently from the rest.\n",
    "\n",
    "Internally, Scikit-Learn uses a library called [joblib](https://joblib.readthedocs.io) to do things in parallel.\n",
    "By default, a process pool is typically used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator = SVC(gamma='auto', random_state=0, probability=True)\n",
    "param_grid = {\n",
    "    'C': [0.001, 10.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-machine parallelism with Dask\n",
    "\n",
    "![](static/sklearn-parallel-dask.png)\n",
    "\n",
    "The Dask and Joblib developers have implemented a [distributed backend](https://joblib.readthedocs.io/en/latest/auto_examples/parallel/distributed_backend_simple.html#sphx-glr-auto-examples-parallel-distributed-backend-simple-py) for joblib so that a Dask cluster can be used to parallelize scikit-learn.\n",
    "To use it, you need to\n",
    "\n",
    "1. Connect to a `Client`\n",
    "2. Make sure your `.fit` call occurs in a `joblib.parallel_backend` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "\n",
    "cluster = KubeCluster(n_workers=10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That grid search fit 8 models. It'd be silly to parallelize that on a cluster. Let's try it on a larger problem (more hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.1, 1.0, 2.5, 5, 10.0],\n",
    "    'kernel': ['rbf', 'poly', 'linear'],\n",
    "    'shrinking': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Estimators\n",
    "\n",
    "Sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on dask arrays and dataframes that may be larger than your machine's RAM.\n",
    "\n",
    "Some scikit-learn estimators can be trained *incrementally*, on batches of data at a time. As new data arrives, the learned parameters are updated, taking the previous values into account. Instead of using `estimator.fit`, you use `estimator.partial_fit`.  This dovetails with Dask Array's and Dask DataFrame's blocked structure: we can iterate through the blocks of a large dask array to incrementally train a model on a large dataset.\n",
    "\n",
    "[`dask_ml.wrappers.Incremental`](http://ml.dask.org/modules/generated/dask_ml.wrappers.Incremental.html#dask_ml.wrappers.Incremental) provides a nice bridge between a larger-than-memory Dask Array and these incremental meta-estiamtors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask_ml.datasets\n",
    "import dask_ml.wrappers\n",
    "import sklearn.linear_model\n",
    "from distributed.utils import format_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dask_ml.datasets.make_classification(n_samples=100_000_000, n_features=30, chunks=1_000_000)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_bytes(X.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dask.persist(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Large Example\n",
    "\n",
    "Use `dask_ml.wrappers.Incremental` to wrap an `sklearn.linear_model.SGDClassifier` to fit the model incrementally.\n",
    "See http://ml.dask.org/incremental.html and http://ml.dask.org/modules/generated/dask_ml.wrappers.Incremental.html#dask_ml.wrappers.Incremental if you need guidance.\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. Incremental is a meta-estimator. It should wrap a scikit-learn estimator like `sklearn.linear_model.SGDClassifier`.\n",
    "2. `Incremental(estimator).fit(X, y, **fit_kwargs)` requires that you pass all the arguments required for `estimator.partial_fit(X, y, **fit_kwargs)` (check the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "inc = dask_ml.wrappers.Incremental(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/07-machine-learning-incremental.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "\n",
    "1. What pattern do you notice on the task stream? Is this computation distributed? Is it parallel?\n",
    "2. Check the return types of `Incremental.predict` and `Incremental.score`. Are these operations eager or lazy? Are they serial or parallel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inc.predict(X)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms implemented in Dask-ML are scalable. They handle larger-than-memory datasets just fine.\n",
    "\n",
    "They follow the scikit-learn API, so if you're familiar with scikit-learn, you'll feel at home with Dask-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subset = X[::10_000]\n",
    "plt.scatter(subset[:, 0], subset[:, 1], c=clf.labels_[::10_000], alpha=0.25, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "Additional resources at http://ml.dask.org."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
